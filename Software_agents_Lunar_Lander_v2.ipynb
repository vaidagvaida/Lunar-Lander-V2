{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software agents -  Lunar lander v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gediminas Sadaunykas and Vaida Gulbinskaite\n",
    "\n",
    "*** Aim:***  Navigate a Lunar Lander to its landing strip. \n",
    "\n",
    "***Agent's Algorithm:***  Deep Q learning\n",
    "\n",
    "***NOTE:*** *Please install OpenAi Gym and keras in order to run this* \n",
    "\n",
    "https://discuss.openai.com/t/installing-openai-gym-universe-on-windows/2092\n",
    "\n",
    "*please install pyglet version 1.2.4 instead of 1.3 (otherwise visuals do not work!!)* \n",
    "\n",
    "______________________________________________________________________________________________________________________\n",
    "***Code testing note:*** *Please note that all number of episodes were changed to 1 for ease of running (otherwise,  this code takes about 3 weeks to run fully)* \n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Load all libraries necesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras import  * \n",
    "from keras.layers.core import Dense ##thought the above should have imported everything but apparently didn't -_- oh well! \n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import six\n",
    "%matplotlib inline \n",
    "import time\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "tall_st = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Srep 2:  Load environment from OpenAi Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Box(8,)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "##Will use https://gym.openai.com/envs/LunarLander-v2/\n",
    "env = gym.make('LunarLander-v2') #4 actions 8 states\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term Dictionary\n",
    "\n",
    "**Code:**   https://keon.io/deep-q-learning/  adapted to solve discrete Lunar lander environment\n",
    "\n",
    "* **Dense()**  =  basic form of NN layer\n",
    "\n",
    "* **Sequential()**  =  Creates the foundation of NN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Please note:  initial neural network is done with 30 neurons in each layer and \n",
    "\n",
    "class LLAgent:\n",
    "    def __init__(self, state_size, action_size, gamma, epsilon, epsilon_decay, lrn_rate, neurons_1stlayer=30,\n",
    "                neurons_2ndlayer=30, activation_1stlayer='relu', activation_2ndlayer='relu'): ## Set hyperparameters \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.gamma = gamma    # discount rate (used to calculate the future discounted reward)\n",
    "        self.epsilon = epsilon  # exploration rate (the rate in which an agent randomly decides its action rather than prediction)\n",
    "        self.epsilon_min = 0.01 ## explore at leasr this much\n",
    "        self.epsilon_decay = epsilon_decay # keep decreasing the number of explorations as the agent gets better\n",
    "        self.lrn_rate = lrn_rate # learnong rate (how much model learns each iteration)\n",
    "        self.DQL_MODEL = self._build_DQL_MODEL()\n",
    "        self.target_DQL_MODEL = self._build_DQL_MODEL()\n",
    "        self.update_target_DQL_MODEL()\n",
    "\n",
    "    def _huber_loss(self, target, prediction): ## Loss function\n",
    "        error = prediction - target\n",
    "        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n",
    "\n",
    "    def _build_DQL_MODEL(self,neurons_1stlayer=30, neurons_2ndlayer=30, activation_1stlayer='relu', activation_2ndlayer='relu'):## Neural Net for Deep-Q learning model\n",
    "        DQL_MODEL = Sequential()\n",
    "        DQL_MODEL.add(Dense(neurons_1stlayer, input_dim=self.state_size, activation=activation_1stlayer)) # Input Layer w/ state size(4) and Hidden Layer w/ 30 n's\n",
    "        DQL_MODEL.add(Dense(neurons_2ndlayer, activation=activation_2ndlayer)) # hidden layer with 30 nodes\n",
    "        DQL_MODEL.add(Dense(self.action_size, activation='linear')) # output Layer:  number of actions with 4 nodes \n",
    "        DQL_MODEL.compile(loss=self._huber_loss, optimizer=Adam(lr=self.lrn_rate))\n",
    "        return DQL_MODEL\n",
    "\n",
    "    def update_target_DQL_MODEL(self): ## Copy weights from DQL_MODEL to target_DQL_MODELff\n",
    "        self.target_DQL_MODEL.set_weights(self.DQL_MODEL.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done): ## Append state, action, reward, and next state to the memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state): ## How the agent decides to act\n",
    "        if np.random.rand() <= self.epsilon: #  acts randomly with constraint of epsilon (exploration vs exploitation)\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.DQL_MODEL.predict(state) # predicts the reward value based on the given state\n",
    "        return np.argmax(act_values[0]) # returns the action based on predicted reward\n",
    "\n",
    "    def replay(self, batch_size): ## Trains the neural net with experiences in the memory\n",
    "        minibatch = random.sample(self.memory, batch_size) # randomly sampled elements of the memories of size batch_size \n",
    "        for state, action, reward, next_state, done in minibatch: # extract informations from each memory\n",
    "            target = self.DQL_MODEL.predict(state) \n",
    "            if done:\n",
    "                target[0][action] = reward  # Target reward\n",
    "            else:  #predict the future discounted reward\n",
    "                a = self.DQL_MODEL.predict(next_state)[0]\n",
    "                t = self.target_DQL_MODEL.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.DQL_MODEL.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min: # when min exploration reached,apply exploration decay\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.DQL_MODEL.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.DQL_MODEL.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_learning(state_size, action_size,gamma, epsilon, epsilon_decay, lrn_rate,  neurons_1stlayer,\n",
    "                neurons_2ndlayer, activation_1stlayer, activation_2ndlayer, episodes=1000):\n",
    "    agent = LLAgent(state_size, action_size, gamma, epsilon,epsilon_decay, lrn_rate,neurons_1stlayer,\n",
    "                neurons_2ndlayer, activation_1stlayer, activation_2ndlayer)\n",
    "    results = {}\n",
    "    reward_avg=0\n",
    "    for e in range(episodes):\n",
    "        #print('episode=', e)\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        #agent.load(\"Software agents/LunarLander-dql_%s_%s_%s_%s_%s.h5\" % (gamma, epsilon, epsilon_min, epsilon_decay, lrn_rate)) #load weights\n",
    "        frames=0 # frame counter\n",
    "        reward_episode=0 # reward counter\n",
    "        done=False # false if episode not done\n",
    "        while not done: #if not landed\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_episode+=reward # Get total reward which could be used as score function\n",
    "            #print(reward)\n",
    "            frames+=1\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.update_target_DQL_MODEL()  # update target\n",
    "                results[e] = reward_episode #update reward for the episode\n",
    "                print(\"episode: {}/{},frames:{},reward_episode:{}\"\n",
    "                      .format(e, episodes, frames, reward_episode)) #display episodic updates\n",
    "                #plt.imshow(env.render(mode='rgb_array')) # display animation Only works with downgraded pyglet \n",
    "            if e==1000: #Stop if agent takes more than 1000 frames to make the algorithm faster and avoid the agent just floating around for a long while \n",
    "                break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size) #re-sample agent memory\n",
    "        if e>(e-100):\n",
    "            reward_avg=reward_episode/100 # The score of the episode is defined as an average of the last 100 frames\n",
    "        #if e % 10 == 0:\n",
    "         #    agent.save(\"Software agents/LunarLander-dql_%s_%s_%s_%s_%s.h5\" % (gamma, epsilon, epsilon_min, epsilon_decay, lrn_rate)) #Save weights\n",
    "    return reward_avg, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1,frames:112,reward_episode:-133.46327640113435\n"
     ]
    }
   ],
   "source": [
    "# Default run\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 128 # deepmind reference\n",
    "\n",
    "\n",
    "default_score, default_results=run_learning(state_size, action_size,gamma=0.99, epsilon =  0.8, epsilon_decay = 0.999, lrn_rate = 0.00025,  \n",
    "                           neurons_1stlayer=30,neurons_2ndlayer=30, activation_1stlayer  = 'relu', activation_2ndlayer= 'relu',episodes  = 1) #episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to dataframe\n",
    "s = pd.Series(default_results, name='Score')\n",
    "df_initial = pd.DataFrame(s)\n",
    "#Rolling mean to smoothe the curve\n",
    "#roll = df_initial.rolling(1000, min_periods=100, freq=None, center=False, win_type=None, on=None, axis=0, closed=None)\n",
    "#test = df_initial.rolling(5000, win_type='triang').mean()\n",
    "#df_initial.to_csv(\"/Software agents/Initial_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Grid search (Searching for the best parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1,frames:63,reward_episode:-266.00610505123325\n",
      "episode: 0/1,frames:51,reward_episode:-395.71406466871446\n",
      "episode: 0/1,frames:55,reward_episode:-227.744211491189\n",
      "episode: 0/1,frames:54,reward_episode:-472.70384547118607\n",
      "episode: 0/1,frames:99,reward_episode:-421.5338671098605\n",
      "episode: 0/1,frames:63,reward_episode:-610.1420307269036\n",
      "episode: 0/1,frames:57,reward_episode:-153.1558271916352\n",
      "episode: 0/1,frames:69,reward_episode:-210.8326058848172\n",
      "episode: 0/1,frames:82,reward_episode:-603.8859190058254\n",
      "episode: 0/1,frames:87,reward_episode:-592.6088803791188\n",
      "episode: 0/1,frames:76,reward_episode:-431.91237590965727\n",
      "episode: 0/1,frames:59,reward_episode:-439.8487388462527\n",
      "episode: 0/1,frames:69,reward_episode:-144.57662967517615\n",
      "episode: 0/1,frames:89,reward_episode:-133.5989421688223\n",
      "episode: 0/1,frames:69,reward_episode:-197.37580634866504\n",
      "episode: 0/1,frames:81,reward_episode:-249.9721585621148\n",
      "episode: 0/1,frames:102,reward_episode:-461.00446551580757\n",
      "episode: 0/1,frames:107,reward_episode:-426.12662461654674\n",
      "episode: 0/1,frames:104,reward_episode:-659.3307885590356\n",
      "episode: 0/1,frames:60,reward_episode:-184.22327552302892\n",
      "episode: 0/1,frames:76,reward_episode:-138.34877357273334\n",
      "episode: 0/1,frames:72,reward_episode:-270.4924419747196\n",
      "episode: 0/1,frames:69,reward_episode:-579.7948591420259\n",
      "episode: 0/1,frames:161,reward_episode:-889.228476177094\n",
      "episode: 0/1,frames:98,reward_episode:-252.27293082169538\n",
      "episode: 0/1,frames:95,reward_episode:-697.6919584639554\n",
      "episode: 0/1,frames:75,reward_episode:-487.9096905531122\n",
      "episode: 0/1,frames:85,reward_episode:-169.42666496852723\n",
      "episode: 0/1,frames:63,reward_episode:-300.21085851655357\n",
      "episode: 0/1,frames:105,reward_episode:-598.7046378460941\n",
      "episode: 0/1,frames:91,reward_episode:-283.63021443101854\n",
      "episode: 0/1,frames:113,reward_episode:-210.16242533091182\n",
      "episode: 0/1,frames:97,reward_episode:-516.3731677685258\n",
      "episode: 0/1,frames:66,reward_episode:-154.57538985923517\n",
      "episode: 0/1,frames:84,reward_episode:-183.17112251586278\n",
      "episode: 0/1,frames:87,reward_episode:-289.98991448948817\n",
      "episode: 0/1,frames:249,reward_episode:-1339.0320874441206\n",
      "episode: 0/1,frames:49,reward_episode:-467.3586790143438\n",
      "episode: 0/1,frames:85,reward_episode:-569.2000517559816\n",
      "episode: 0/1,frames:50,reward_episode:-421.22598523567393\n",
      "episode: 0/1,frames:61,reward_episode:-463.9085830776211\n",
      "episode: 0/1,frames:78,reward_episode:-769.402704224844\n",
      "episode: 0/1,frames:78,reward_episode:-445.4150711600416\n",
      "episode: 0/1,frames:102,reward_episode:-371.0193450237332\n",
      "episode: 0/1,frames:66,reward_episode:-340.4187259455847\n",
      "episode: 0/1,frames:167,reward_episode:-79.37028662636362\n",
      "episode: 0/1,frames:73,reward_episode:-488.14864908900717\n",
      "episode: 0/1,frames:90,reward_episode:-527.5634004599951\n",
      "episode: 0/1,frames:91,reward_episode:-139.62878035882432\n",
      "episode: 0/1,frames:87,reward_episode:-186.22479095861883\n",
      "episode: 0/1,frames:116,reward_episode:-38.57970643558636\n",
      "episode: 0/1,frames:73,reward_episode:-138.01647593115527\n",
      "episode: 0/1,frames:77,reward_episode:-237.81592948912785\n",
      "episode: 0/1,frames:83,reward_episode:-217.13621577280543\n"
     ]
    }
   ],
   "source": [
    "###Run via google cloud###\n",
    "#Full grid could not be run due to computational issues but we tried our best\n",
    "#Env constants definition\n",
    "state_size = env.observation_space.shape[0]\n",
    "batch_size = 128 # deepmind reference\n",
    "\n",
    "\n",
    "gamma_grid=[0.1,0.5,0.9]\n",
    "epsilon_grid=[0.1,0.5,0.9]\n",
    "epsilon_grid=[0.1,0.5,0.9]\n",
    "epsilon_decay_grid=[0.99,0.5,0.1]\n",
    "lrn_rate_grid=[0.0001,0.01]\n",
    "\n",
    "\n",
    "score_dictionary1={}\n",
    "Grid_time_st = time.time()\n",
    "\n",
    "for gamma in gamma_grid:\n",
    "    for epsilon in epsilon_grid:\n",
    "        for epsilon_decay in epsilon_decay_grid:\n",
    "            for lrn_rate in lrn_rate_grid:\n",
    "                Iter_time_st=time.time() # Time start iteration\n",
    "                ##parameters used\n",
    "                score, results=run_learning(state_size, action_size, gamma, epsilon, epsilon_decay, lrn_rate, neurons_1stlayer=30,\n",
    "            neurons_2ndlayer=30, activation_1stlayer='relu', activation_2ndlayer='relu',episodes = 1)# episodes=20000) ##Initian NN structure was used for this\n",
    "                Iter_time_en = time.time()\n",
    "                Duration_iter=Iter_time_en-Iter_time_st\n",
    "                #print('Gamma: %s ; Epsilon: %s; Epsilon_decay: %s; Lrn_rate: %s; Score: score; Duration: %s' \n",
    "                #     % (gamma, epsilon, epsilon_decay, lrn_rate, score, Duration_iter))\n",
    "                #print('-'*80)\n",
    "                score_dictionary1[(gamma, epsilon, epsilon_decay, lrn_rate)]=score \n",
    "                s = pd.Series(results, name='Score')\n",
    "                df = pd.DataFrame(s)\n",
    "       #         df.to_csv(\"/Software Agents/Results_Gamma%s_Epsilon%s_Epsdecay%s_LrnRate%s.csv\" % (gamma, epsilon,  epsilon_decay, lrn_rate))\n",
    "##All exported to excel files,  which were later used to create grid parameter table\n",
    "#To say that one model was the best over other was unfair because we could not run the full grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Neural network Grid search (Searching for the optiman number of neurons and best activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change neural net sizes and activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1,frames:119,reward_episode:-74.00599787266466\n",
      "episode: 0/1,frames:100,reward_episode:-161.23227653720195\n",
      "episode: 0/1,frames:69,reward_episode:-466.2225772356683\n",
      "episode: 0/1,frames:98,reward_episode:-398.47044658822375\n",
      "episode: 0/1,frames:123,reward_episode:-176.8107512949634\n",
      "episode: 0/1,frames:57,reward_episode:-322.9454130921479\n",
      "episode: 0/1,frames:115,reward_episode:-114.75757710301312\n",
      "episode: 0/1,frames:115,reward_episode:-187.60484029009888\n",
      "episode: 0/1,frames:87,reward_episode:-555.8039231867073\n"
     ]
    }
   ],
   "source": [
    "###Run via google cloud###\n",
    "neurons_layer_grid=[6,25,50] \n",
    "activation_layer_grid=['sigmoid', 'relu' ,'leaky_relu']\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "#Grid search with base parameters\n",
    "gamma = 0.99 \n",
    "epsilon =  0.8\n",
    "epsilon_decay = 0.999\n",
    "lrn_rate = 0.00025\n",
    "\n",
    "\n",
    "\n",
    "score_dictionary_4={}\n",
    "episodes = {} ## Store the episode information:  Key = Episode,  Value = (episode, score,  epsilon) \n",
    "if __name__ == \"__main__\":\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = LLAgent(state_size, action_size,state_size, action_size, gamma, epsilon, epsilon_decay, lrn_rate)\n",
    "    done = False\n",
    "    batch_size = 128\n",
    "     \n",
    "        \n",
    "for neurons_layer in neurons_layer_grid:\n",
    "    for activation_layer in activation_layer_grid:\n",
    "        score, results=run_learning(state_size, action_size, gamma, epsilon, epsilon_decay, lrn_rate,neurons_layer,neurons_layer, activation_layer, activation_layer,episodes = 1) #episodes=10000)\n",
    "        # Use parameters from grid search\n",
    "        score_dictionary1[(gamma, epsilon, epsilon_decay, lrn_rate)]=score \n",
    "        s = pd.Series(results, name='Score')\n",
    "        df = pd.DataFrame(s)\n",
    "#        df.to_csv(\"/Users/vgulbi/Dropbox/Team Lithuania/Software agents/BEST_Results_layer%s_Activation_%s.csv\" % (neurons_layer, activation_layer))\n",
    "##All exported to excel and used to create grid search table as well as determine the best model                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Best parameters selecred from the grid search table (where average of the last 100 episodes was the largest)\n",
    "# Best of grid search\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 128 # deepmind reference\n",
    "\n",
    "\n",
    "default_score, default_results=run_learning(state_size, action_size,gamma=0.99, epsilon =  0.8, epsilon_decay = 0.999, lrn_rate = 0.00025,  \n",
    "                           neurons_1stlayer=50,neurons_2ndlayer=50, activation_1stlayer  = 'sigmoid', activation_2ndlayer= 'sigmoid', episodes  = 1)#, episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to dataframe\n",
    "s = pd.Series(default_results, name='Score')\n",
    "df_initial = pd.DataFrame(s)\n",
    "#Rolling mean to smoothe the curve\n",
    "#roll = df_initial.rolling(1000, min_periods=100, freq=None, center=False, win_type=None, on=None, axis=0, closed=None)\n",
    "#test = df_initial.rolling(5000, win_type='triang').mean()\n",
    "#df_initial.to_csv(\"/Software agents/Initial_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
